{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2936206, 8)\n",
      "Test shape: (734039, 8)\n",
      "After dropna:\n",
      "Train shape: (2936206, 8)\n",
      "Test shape: (734039, 8)\n"
     ]
    }
   ],
   "source": [
    "train_path = \"data/train.parquet\"\n",
    "test_path  = \"data/test.parquet\"\n",
    "\n",
    "train_df = pd.read_parquet(train_path)\n",
    "test_df  = pd.read_parquet(test_path)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "train_df = train_df.dropna(subset=[\"text\", \"label\"]).copy()\n",
    "test_df  = test_df.dropna(subset=[\"text\", \"label\"]).copy()\n",
    "\n",
    "print(\"After dropna:\")\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "Neutral            1507697\n",
       "Slightly Biased     831621\n",
       "Highly Biased       596888\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Unique labels:\", train_df[\"label\"].nunique())\n",
    "display(train_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2936206\n",
      "Test samples: 734039\n",
      "Example text: http://twitpic.com/6996z - Todays is Katie's birthday. This is what dad got her...(for the weekend\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)   # collapse whitespace\n",
    "    return text\n",
    "\n",
    "X_train = train_df[\"text\"].astype(str).map(clean_text)\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "X_test  = test_df[\"text\"].astype(str).map(clean_text)\n",
    "y_test  = test_df[\"label\"]\n",
    "\n",
    "print(\"Train samples:\", len(X_train))\n",
    "print(\"Test samples:\", len(X_test))\n",
    "print(\"Example text:\", X_train.iloc[0][:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(max_features=30000, min_df=5,\n",
      "                                 stop_words='english')),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000,\n",
      "                                    n_jobs=-1, solver='saga', verbose=1))])\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 1),   # ✅ unigrams first (fast)\n",
    "        max_features=30000,   # ✅ reduce features for speed\n",
    "        min_df=5              # ✅ ignore rare words\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"saga\",        # ✅ supports multiclass + sparse + can be faster\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshayprajapati/Desktop/News_Bias/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1\n",
      "Epoch 2, change: 0.18550032\n",
      "Epoch 3, change: 0.13342202\n",
      "Epoch 4, change: 0.080878038\n",
      "Epoch 5, change: 0.062664543\n",
      "Epoch 6, change: 0.05670166\n",
      "Epoch 7, change: 0.044103118\n",
      "Epoch 8, change: 0.035301526\n",
      "Epoch 9, change: 0.020614973\n",
      "Epoch 10, change: 0.037809432\n",
      "Epoch 11, change: 0.018289194\n",
      "Epoch 12, change: 0.010621955\n",
      "Epoch 13, change: 0.010203473\n",
      "Epoch 14, change: 0.010101023\n",
      "Epoch 15, change: 0.0062356083\n",
      "Epoch 16, change: 0.0089830536\n",
      "Epoch 17, change: 0.0060612349\n",
      "Epoch 18, change: 0.0026581263\n",
      "Epoch 19, change: 0.0017270819\n",
      "Epoch 20, change: 0.0012780312\n",
      "Epoch 21, change: 0.00097545237\n",
      "Epoch 22, change: 0.00083280889\n",
      "Epoch 23, change: 0.00054937167\n",
      "Epoch 24, change: 0.00028750912\n",
      "Epoch 25, change: 0.00041992446\n",
      "Epoch 26, change: 0.00021588791\n",
      "Epoch 27, change: 0.0002226491\n",
      "Epoch 28, change: 0.00024482373\n",
      "Epoch 29, change: 0.00011933025\n",
      "convergence after 30 epochs took 83 seconds\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7958078521713424\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Highly Biased       0.75      0.82      0.78    148777\n",
      "        Neutral       0.90      0.86      0.88    377055\n",
      "Slightly Biased       0.65      0.66      0.66    208207\n",
      "\n",
      "       accuracy                           0.80    734039\n",
      "      macro avg       0.77      0.78      0.77    734039\n",
      "   weighted avg       0.80      0.80      0.80    734039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(model, \"models/tfidf_logreg.joblib\")\n",
    "print(\"Model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
